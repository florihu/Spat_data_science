{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Decison for indicator\n",
    "\n",
    "Secondary: NV.IND.TOTL.ZS https://data.worldbank.org/indicator/NV.IND.TOTL.ZS?view=chart\n",
    "Primary: ER.H2O.FWIN.ZS https://data.worldbank.org/indicator/ER.H2O.FWIN.ZS\n",
    "\n",
    "The basic assumption is that a similar content of % GDP of industry lead to a similar & of total water withdrawl of industry\n",
    "\n",
    "Here we load the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-12T13:36:28.999934700Z",
     "start_time": "2023-10-12T13:36:24.525724100Z"
    }
   },
   "outputs": [],
   "source": [
    "import wbgapi as wb\n",
    "\n",
    "\n",
    "data = wb.data.DataFrame(['ER.H2O.FWIN.ZS','NV.IND.TOTL.ZS'], time=[2020] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ER.H2O.FWIN.ZS  NV.IND.TOTL.ZS\n",
      "    name                                        \n",
      "ABW Aruba                    NaN             NaN\n",
      "AFG Afghanistan         0.831988       14.031699\n",
      "AGO Angola             33.947294       45.678242\n",
      "ALB Albania             2.035623       20.114530\n",
      "AND Andorra                  NaN       12.660158\n",
      "...                          ...             ...\n",
      "XKX Kosovo                   NaN       27.558457\n",
      "YEM Yemen, Rep.         1.823282             NaN\n",
      "ZAF South Africa       21.313440       23.272787\n",
      "ZMB Zambia              8.269720       40.257899\n",
      "ZWE Zimbabwe            2.157087       32.767517\n",
      "\n",
      "[217 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "# Get information about all economies (countries and regions)\n",
    "#all_economies = wb.economy.Series(id='all')\n",
    "def region_cleaner(data):\n",
    "    economies = wb.economy.DataFrame()\n",
    "    aggregates_id = economies[economies['aggregate'] == True]\n",
    "    data_clean = data.loc[data.index.difference(aggregates_id.index)]\n",
    "\n",
    "    data_clean_name = data_clean.join(economies['name'], on = data_clean.index, how = 'inner')\n",
    "    data_clean_name.set_index('name', append=True, inplace =True)\n",
    "\n",
    "    data_clean_name =data_clean_name.drop(columns='key_0')\n",
    "\n",
    "    return data_clean_name\n",
    "\n",
    "def nan_checker(data):\n",
    "    return data.isna().sum()\n",
    "\n",
    "\n",
    "print(region_cleaner(data))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T13:36:32.799697300Z",
     "start_time": "2023-10-12T13:36:29.005985600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    ER.H2O.FWIN.ZS  NV.IND.TOTL.ZS\n",
      "    name                                                          \n",
      "AFG Afghanistan                           0.831988       14.031699\n",
      "AGO Angola                               33.947294       45.678242\n",
      "ALB Albania                               2.035623       20.114530\n",
      "ARE United Arab Emirates                  0.742776       42.174043\n",
      "ARG Argentina                            10.587612       22.180573\n",
      "...                                            ...             ...\n",
      "VCT St. Vincent and the Grenadines        0.023524       12.597134\n",
      "VNM Vietnam                               3.747409       36.743712\n",
      "ZAF South Africa                         21.313440       23.272787\n",
      "ZMB Zambia                                8.269720       40.257899\n",
      "ZWE Zimbabwe                              2.157087       32.767517\n",
      "\n",
      "[171 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def set_split(data_not_clean):\n",
    "    data = region_cleaner(data_not_clean)\n",
    "    data_new = data[~data['NV.IND.TOTL.ZS'].isna() & ~data['ER.H2O.FWIN.ZS'].isna()]\n",
    "    #data_new=  data[~data['NV.IND.TOTL.ZS'].isna() & data['ER.H2O.FWIN.ZS'].isna()]\n",
    "\n",
    "    return data_new\n",
    "\n",
    "\n",
    "print(set_split(data))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T13:36:35.792762700Z",
     "start_time": "2023-10-12T13:36:32.811797800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'training_data'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\Documents\\GitHub\\Sustanal\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3789\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3790\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcasted_key\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3791\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32mindex.pyx:152\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mindex.pyx:181\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'training_data'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 22\u001B[0m\n\u001B[0;32m     18\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m     19\u001B[0m     plt\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m---> 22\u001B[0m \u001B[43mplot_stuff\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[4], line 6\u001B[0m, in \u001B[0;36mplot_stuff\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_stuff\u001B[39m(data):\n\u001B[1;32m----> 6\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mset_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtraining_data\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m      7\u001B[0m     industry_share \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNV.IND.TOTL.ZS\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      8\u001B[0m     water_share \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mER.H2O.FWIN.ZS\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Sustanal\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:3896\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3894\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mnlevels \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m   3895\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_getitem_multilevel(key)\n\u001B[1;32m-> 3896\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3897\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_integer(indexer):\n\u001B[0;32m   3898\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m [indexer]\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\Sustanal\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[1;34m(self, key)\u001B[0m\n\u001B[0;32m   3792\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(casted_key, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (\n\u001B[0;32m   3793\u001B[0m         \u001B[38;5;28misinstance\u001B[39m(casted_key, abc\u001B[38;5;241m.\u001B[39mIterable)\n\u001B[0;32m   3794\u001B[0m         \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mslice\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m casted_key)\n\u001B[0;32m   3795\u001B[0m     ):\n\u001B[0;32m   3796\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InvalidIndexError(key)\n\u001B[1;32m-> 3797\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(key) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m   3798\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m   3799\u001B[0m     \u001B[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[0;32m   3800\u001B[0m     \u001B[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[0;32m   3801\u001B[0m     \u001B[38;5;66;03m#  the TypeError.\u001B[39;00m\n\u001B[0;32m   3802\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[1;31mKeyError\u001B[0m: 'training_data'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_stuff(data):\n",
    "    data = set_split(data)['training_data']\n",
    "    industry_share = data['NV.IND.TOTL.ZS']\n",
    "    water_share = data['ER.H2O.FWIN.ZS']\n",
    "\n",
    "\n",
    "    sns.scatterplot(x =industry_share, y = water_share)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    sns.histplot(industry_share, kde=True)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    sns.histplot(water_share, kde=True)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "plot_stuff(data)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T13:36:42.103200600Z",
     "start_time": "2023-10-12T13:36:35.792762700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "def imput_calc(data):\n",
    "\n",
    "    data = region_cleaner(data)\n",
    "\n",
    "\n",
    "    y = data['ER.H2O.FWIN.ZS']\n",
    "    X =  data['NV.IND.TOTL.ZS']\n",
    "\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "    data['H20_imputed_simple'] = imputer.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "    knn_imputer_uniform = KNNImputer(n_neighbors=10, weights='uniform')\n",
    "\n",
    "    data['H20_imp_k_uni'] = knn_imputer_uniform.fit_transform(\n",
    "    np.column_stack((X, y)))[:,1]\n",
    "\n",
    "    knn_imputer_dist = KNNImputer(n_neighbors=10, weights='distance')\n",
    "\n",
    "    data['H20_imp_k_dist'] = knn_imputer_dist.fit_transform(\n",
    "    np.column_stack((X, y)))[:,1]\n",
    "\n",
    "\n",
    "    return data\n",
    "\n",
    "print(imput_calc(data))\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.histplot(imput_calc(data).iloc[:,[0,2,3,4]], element='step', common_norm = False, kde = True)\n",
    "plt.savefig('final_assignments_graphs\\histplot.jpeg')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Analysis:\n",
    "1. imputed simple is definetly problematic because it skews the distribution quite tremendously obiously because we take the average. You can assess this best in the skewed distribution H20 in direction of higher values.\n",
    "2.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.scatterplot(np.array(imput_calc(data).iloc[:,2:]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "def imp_valid(data, n_folds=5):\n",
    "\n",
    "    scores ={'aver':{'R2':[], 'NMRSE':[], 'PBIAS':[] },'k_uni':{'R2':[], 'NMRSE':[], 'PBIAS':[] },'k_dist':{'R2':[], 'NMRSE':[], 'PBIAS':[] }}\n",
    "\n",
    "    data = set_split(data)\n",
    "\n",
    "    # Perform imputations and model evaluation over multiple runs\n",
    "\n",
    "    # Initialize a k-fold cross-validation splitter\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    data_copy = data.copy()\n",
    "    y = np.array(data['ER.H2O.FWIN.ZS'])\n",
    "    X = np.array(data['NV.IND.TOTL.ZS'])\n",
    "\n",
    "    for imp_switch in ['aver', 'k_uni', 'k_dist']:\n",
    "\n",
    "        for train_index, test_index in kf.split(data):\n",
    "\n",
    "            # Set NaN values in the 'ER.H2O.FWIN.ZS' column for the test index\n",
    "            y_test = y.copy()\n",
    "            y_test[test_index] = np.NaN\n",
    "\n",
    "            if imp_switch == 'aver':\n",
    "                imputer = SimpleImputer(strategy='mean')\n",
    "                y_imp = imputer.fit_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "            elif imp_switch == 'k_uni':\n",
    "\n",
    "                knn_imputer_uniform = KNNImputer(n_neighbors=10, weights='uniform')\n",
    "                y_imp = knn_imputer_uniform.fit_transform(np.column_stack((X, y_test)))[:,1]\n",
    "\n",
    "            elif imp_switch == 'k_dist':\n",
    "\n",
    "                knn_imputer_dist = KNNImputer(n_neighbors=10, weights='distance')\n",
    "\n",
    "                y_imp = knn_imputer_dist.fit_transform(np.column_stack((X, y_test)))[:,1]\n",
    "\n",
    "\n",
    "            scores[imp_switch]['R2'].append(r2_score(y[test_index], y_imp[test_index]))\n",
    "            scores[imp_switch]['NMRSE'].append(np.sqrt(mean_squared_error(y[test_index],\n",
    "                                                                          y_imp[test_index]))/np.mean(y[test_index]))\n",
    "            scores[imp_switch]['PBIAS'].append(((y_imp[test_index]-y[test_index]).sum()\n",
    "                                                    /y[test_index].sum()))\n",
    "\n",
    "        scores[imp_switch]['R2'] = np.mean(np.array(scores[imp_switch]['R2']))\n",
    "        scores[imp_switch]['NMRSE'] = np.mean(np.array(scores[imp_switch]['NMRSE']))\n",
    "        scores[imp_switch]['PBIAS'] = np.mean(np.array(scores[imp_switch]['PBIAS']))\n",
    "\n",
    "    return pd.DataFrame.from_dict(scores)\n",
    "\n",
    "\n",
    "\n",
    "imp_valid(data)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* R2: Outcome is quite devistating. None of the models fits the real data well. R2 interpretation the variance in the observed data cannot be adressed by the model-> Taking the average of th H20 would be the fit with least variance.\n",
    "* NMRSE is far away from zero which indicates a bad fit\n",
    "* aver is biased towards higher values = overfit. This could be also nicely assessed in the distribution\n",
    "* K_uni and K-kist tend to underfit however they have both a very small bias of <3%\n",
    "* As a result K_uni and K_aver are not biased but are highly affected by fluctuations in the test data\n",
    "* If you have to choose chosse K_uni"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def output_to_xlsx(data):\n",
    "    imp_data = imput_calc(data)\n",
    "    valid_data = imp_valid(data)\n",
    "    # Create an ExcelWriter object and specify the Excel file name\n",
    "    file_name = 'final_assignment_data\\output.xlsx'\n",
    "    with pd.ExcelWriter(file_name) as writer:\n",
    "        imp_data.to_excel(writer, sheet_name='imputed_data')\n",
    "        valid_data.to_excel(writer, sheet_name='validation_imputation')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " sklearn.metrics.r2_score(y_true, y_pred, *, sample_weight=None, multioutput='uniform_average', force_finite=True)[source]¶\n",
    "\n",
    "(coefficient of determination) regression score function.\n",
    "\n",
    "Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). In the general case when the true y is non-constant, a constant model that always predicts the average y disregarding the input features would get a\n",
    "\n",
    "score of 0.0.\n",
    "\n",
    "In the particular case when y_true is constant, the\n",
    "\n",
    "score is not finite: it is either NaN (perfect predictions) or -Inf (imperfect predictions). To prevent such non-finite numbers to pollute higher-level experiments such as a grid search cross-validation, by default these cases are replaced with 1.0 (perfect predictions) or 0.0 (imperfect predictions) respectively. You can set force_finite to False to prevent this fix from happening.\n",
    "\n",
    "Note: when the prediction residuals have zero mean, the\n",
    "score is identical to the Explained Variance score."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def indic_selection(prim = 'ER.H2O.FWIN.ZS', sec = 'NV.IND.TOTL.ZS'):\n",
    "\n",
    "    return {'prim':prim, 'sec':sec }\n",
    "\n",
    "\n",
    "def clustering(data):\n",
    "    data = imput_calc(data)\n",
    "\n",
    "    data.pop(indic_selection()['sec'])\n",
    "\n",
    "\n",
    "    #cat 1: < median -> high sust\n",
    "    #threshold 2: median =< VALUE =< average -> medium sust\n",
    "    #threshold 3: value >average -> high sust\n",
    "    #\n",
    "    thres1 = data[indic_selection()['prim']].describe()['50%']\n",
    "    thres2 = data[indic_selection()['prim']].describe()['75%']\n",
    "\n",
    "    dict_ = {key for key in data.columns}\n",
    "\n",
    "    dict_bar = {}\n",
    "    for column in data.columns:\n",
    "        dict_bar[column] = []\n",
    "        mask_high = data[column] < thres1\n",
    "        mask_medium =   (data[column] >= thres1) & (data[column] <= thres2)\n",
    "        mask_low = data[column] > thres2\n",
    "        dict_bar[column].append(data[column][mask_high].describe()['count'])\n",
    "        dict_bar[column].append(data[column][mask_medium].describe()['count'])\n",
    "        dict_bar[column].append(data[column][mask_low].describe()['count'])\n",
    "\n",
    "    df = pd.DataFrame.from_dict(dict_bar)\n",
    "    df.index = ['high', 'medium','low']\n",
    "    return df\n",
    "\n",
    "\n",
    "result_df = clustering(data)\n",
    "result_df_percentage = result_df.div(result_df.sum(axis=0), axis=1) * 100\n",
    "\n",
    "result_df_percentage.T.plot(kind='bar', stacked=True)\n",
    "plt.ylabel('Percentage (%)')\n",
    "plt.savefig('final_assignments_graphs\\porportion_classes')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from stuff import app\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T13:38:22.106002Z",
     "start_time": "2023-10-12T13:38:14.672192700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
